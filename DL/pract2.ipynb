{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use MNIST Fashion Dataset and create a classifier to classify fashion clothing into\n",
    "categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_j-jTUIGjxog",
    "outputId": "55c7705f-17d3-4a42-bd33-c7f943210796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 4us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 20s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0s/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# There are 10 image classes in this dataset and each class has a mapping corresponding to the following labels:\n",
    "\n",
    "#0 T-shirt/top\n",
    "#1 Trouser\n",
    "#2 pullover\n",
    "#3 Dress\n",
    "#4 Coat\n",
    "#5 sandals\n",
    "#6 shirt\n",
    "#7 sneaker\n",
    "#8 bag\n",
    "#9 ankle boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "EPJO4exOTkPW",
    "outputId": "2881a583-6bb8-46fc-93c3-f54ee9cdf547"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16ebe6f2fa0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFZgYodrtFFw"
   },
   "source": [
    "plt.imshow(): This is a function from the matplotlib library, specifically used to display images. It takes an array-like object as input and renders it as an image.\n",
    "\n",
    "x_train[1]: This accesses the second element (index 1) of the x_train array, which contains the input images from the Fashion MNIST dataset. Each element of x_train is a 2D array representing a grayscale image.\n",
    "\n",
    "So, plt.imshow(x_train[1]) displays the second image from the training set (x_train[1]) as a grayscale image using matplotlib. Since the Fashion MNIST dataset contains grayscale images, imshow() will render the image in grayscale by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "g6gwec8jYnpV",
    "outputId": "1713da35-d39a-4fb9-9cd3-717337d18924"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16ebe720f70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-uAX_9PtO8Q"
   },
   "source": [
    "plt.imshow(): This function, as before, is used to display images using matplotlib.\n",
    "\n",
    "x_train[0]: This retrieves the first element (index 0) of the x_train array, which represents the input image data from the Fashion MNIST dataset. Each element of x_train is a 2D array representing a grayscale image.\n",
    "\n",
    "So, plt.imshow(x_train[0]) displays the first image from the training set (x_train[0]) as a grayscale image using matplotlib. Since the Fashion MNIST dataset contains grayscale images, imshow() will render the image in grayscale by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMqAGmsetHno"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0-c-5JWYPPqT"
   },
   "outputs": [],
   "source": [
    "# Next, we will preprocess the data by scaling the pixel values to be between 0 and 1, and then reshaping the images to be 28x28 pixels.\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# 28, 28 comes from width, height, 1 comes from the number of channels\n",
    "# -1 means that the length in that dimension is inferred.\n",
    "# This is done based on the constraint that the number of elements in an ndarray or Tensor when reshaped must remain the same.\n",
    "\n",
    "# each image is a row vector (784 elements) and there are lots of such rows (let it be n, so there are 784n elements). So TensorFlow can infer that -1 is n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Th54tYxXPyiB",
    "outputId": "2c57f6f9-2e1d-417c-b574-49b6e361805a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting the training_images array to 4 dimensional array with sizes 60000, 28, 28, 1 for 0th to 3rd dimension.\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6TMVZKAtP4BL",
    "outputId": "ecb775ce-92c6-45b0-d01d-624d110f709b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dCYL-C4PP_ki",
    "outputId": "10aa5237-88f0-44db-b154-ded24688b66e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ocddGOxQFiM",
    "outputId": "cdeb3618-f46e-4051-913d-fb31ad7ff0b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "txTuSANLQJ1S"
   },
   "outputs": [],
   "source": [
    "# We will use a convolutional neural network (CNN) to classify the fashion items.\n",
    "# The CNN will consist of multiple convolutional layers followed by max pooling,\n",
    "# dropout, and dense layers. Here is the code for the model:\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    # 32 filters (default), randomly initialized\n",
    "    # 3*3 is Size of Filter\n",
    "    # 28,28,1 size of Input Image\n",
    "    # No zero-padding: every output 2 pixels less in every dimension\n",
    "    # in Paramter shwon 320 is value of weights: (3x3 filter weights + 32 bias) * 32 filters\n",
    "    # 32*3*3=288(Total)+32(bias)= 320\n",
    "\n",
    "\n",
    "    keras.layers.MaxPooling2D((2,2)),\n",
    "    # It shown 13 * 13 size image with 32 channel or filter or depth.\n",
    "\n",
    "    keras.layers.Dropout(0.25),\n",
    "    # Reduce Overfitting of Training sample drop out 25% Neuron\n",
    "\n",
    "    keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    # Deeper layers use 64 filters\n",
    "    # 3*3 is Size of Filter\n",
    "    # Observe how the input image on 28x28x1 is transformed to a 3x3x64 feature map\n",
    "    # 13(Size)-3(Filter Size )+1(bias)=11 Size for Width and Height with 64 Depth or filtter or channel\n",
    "    # in Paramter shwon 18496 is value of weights: (3x3 filter weights + 64 bias) * 64 filters\n",
    "    # 64*3*3=576+1=577*32 + 32(bias)=18496\n",
    "\n",
    "    keras.layers.MaxPooling2D((2,2)),\n",
    "    # It shown 5 * 5 size image with 64 channel or filter or depth.\n",
    "\n",
    "    keras.layers.Dropout(0.25),\n",
    "\n",
    "    keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    # Deeper layers use 128 filters\n",
    "    # 3*3 is Size of Filter\n",
    "    # Observe how the input image on 28x28x1 is transformed to a 3x3x128 feature map\n",
    "    # It show 5(Size)-3(Filter Size )+1(bias)=3 Size for Width and Height with 64 Depth or filtter or channel\n",
    "    # 128*3*3=1152+1=1153*64 + 64(bias)= 73856\n",
    "\n",
    "    # To classify the images, we still need a Dense and Softmax layer.\n",
    "    # We need to flatten the 3x3x128 feature map to a vector of size 1152\n",
    "    # https://medium.com/@iamvarman/how-to-calculate-the-number-of-parameters-in-the-cnn-5bd55364d7ca\n",
    "\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    # 128 Size of Node in Dense Layer\n",
    "    # 1152*128 = 147584\n",
    "\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "    # 10 Size of Node another Dense Layer\n",
    "    # 128*10+10 bias= 1290\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WzBjU3g3idgx",
    "outputId": "5f190c33-9156-463f-ad66-f90dc6a42e8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 13, 13, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 5, 5, 64)          0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 3, 128)         73856     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1152)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               147584    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 241,546\n",
      "Trainable params: 241,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p8ziWNG1QTJh",
    "outputId": "a80643b4-e92d-4219-d647-3442490d4234"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 54s 27ms/step - loss: 0.5724 - accuracy: 0.7858 - val_loss: 0.3757 - val_accuracy: 0.8620\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.3712 - accuracy: 0.8638 - val_loss: 0.3395 - val_accuracy: 0.8729\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.3210 - accuracy: 0.8822 - val_loss: 0.2969 - val_accuracy: 0.8942\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.2952 - accuracy: 0.8912 - val_loss: 0.2805 - val_accuracy: 0.8947\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 52s 28ms/step - loss: 0.2742 - accuracy: 0.8981 - val_loss: 0.2626 - val_accuracy: 0.9029\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 52s 28ms/step - loss: 0.2619 - accuracy: 0.9021 - val_loss: 0.2720 - val_accuracy: 0.9000\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 53s 28ms/step - loss: 0.2519 - accuracy: 0.9057 - val_loss: 0.2544 - val_accuracy: 0.9089\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 53s 28ms/step - loss: 0.2420 - accuracy: 0.9088 - val_loss: 0.2545 - val_accuracy: 0.9087\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 53s 28ms/step - loss: 0.2333 - accuracy: 0.9123 - val_loss: 0.2581 - val_accuracy: 0.9080\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 54s 29ms/step - loss: 0.2259 - accuracy: 0.9159 - val_loss: 0.2636 - val_accuracy: 0.9076\n"
     ]
    }
   ],
   "source": [
    "# Compile and Train the Model\n",
    "# After defining the model, we will compile it and train it on the training data.\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# 1875 is a number of batches. By default batches contain 32 samles.60000 / 32 = 1875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BhG8LcfRQnQl",
    "outputId": "683ddb44-7d10-447b-f7c0-a98ab8cb44d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 11ms/step - loss: 0.2636 - accuracy: 0.9076\n",
      "Test accuracy: 0.9075999855995178\n"
     ]
    }
   ],
   "source": [
    "# Finally, we will evaluate the performance of the model on the test data.\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
